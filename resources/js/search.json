[[{"l":"Home","p":["Harpy is a haplotagging data processing pipeline for Linux-based systems. It uses all the magic of Snakemake under the hood to handle the worklfow decision-making, but as a user, you just interact with it like a normal command-line program. Harpy uses both well known and niche programs to take raw haplotagging sequences and process them to become called SNP genotypes (or haplotypes). Most of the settings are pre-configured and the settings you can modify are done at the command line. There aren't too many, which should make things a little simpler."]},{"l":"Modules","p":["Harpy is modular, meaning you can use different parts of it independent from each other. Need to only align reads? Great! Only want to call variants? Awesome! All modules are called by harpy modulename. For example, use harpy align to align reads.","Module","Description","trim","Remove adapters and quality trim sequences","align","Align sample sequences to a reference genome","variants","Call variants from sample alignments","impute","Impute genotypes from genotype likelihoods","phase","Phase SNPs into haplotypes","You can call harpy without any arguments (or with --help) to print the docstring to your terminal. You can likewise call any of the modules with --help(e.g. harpy align --help) to see their usage."]}],[{"l":"Install HARPY","p":["A working installation of conda. We recommend you use mamba. It's so much faster and uses a lot less memory.","Until this pipeline gets completed and hosted on Bioconda, it will be available by cloning/downloading the repository. The dependencies can be installed into a conda environment using the provided harpyenv.yaml:","The version of EMA bundled in this repository ( ema-h) is a fork of the orignal EMA modified to work with Generation 1 haplotag beadtags (AxxCxxBxxDxx)."]}],[{"l":"Software used in HARPY","p":["bash","bcftools","bgzip","bwa","click","conda","EMA","fastp","HapCUT2","HARPY is the sum of its parts, and out of tremendous respect for the developers involved in the included software, we would like to highlight the tools directly involved in HARPY's many moving pieces.","Issues with specific tools might warrant a discussion with the authors/developers on the repositories of these projects.","LEVIATHAN","LRez","mamba","pubication","Publication","python","rich-click","sambamba","samtools","Snakemake","Software","STITCH","Website"]}],[{"l":"Generate Extra Files","p":["While Harpy needs basic files like a reference genome and fastq sequences, some of the steps also optionally or strictly require extra files. You can create various files necessary for different modules using the Harpy init module:","The arguments represent different sub-commands and can be run in any order or combination to generate the files you need."]},{"l":"Running Options","p":["--help","--hpc","--popgroup","--stitch-params","-h","-p","-s","all the samples will be assigned to group 1 since file names don't always provide grouping information, so make sure to edit the second column to reflect your data correctly.","argument","Create generic sample-group file using existing sample file names (fq.gz or bam)","Create HPC scheduling profile for cluster submission","Create template STITCH parameter file","default","description","file path","folder path","For snakemake to work in harmony with an HPC scheduler, a \"profile\" needs to be provided that tells Snakemake how it needs to interact with the HPC scheduler to submit your jobs to the cluster. Using harpy init --hpc hpc-type will create the necessary folder and profile yaml file for you to use. To use the profile, call the intended Harpy module with an extra ``--snakemake` argument:","no","required","short name","Show the module docstring","slurm","slurm: by jdblischak","string [slurm]","takes the format of sample<tab>group","the file will look like:","These are ongoing as I figure out how it works. Currently working on SLURM support, as that is the system we have available.","This file is entirely optional and useful if you want variant calling to happen on a per-population level using mpileup via harpy variants -p.","type","Typically, one runs STITCH multiple times, exploring how results vary with different model parameters. The solution Harpy uses for this is to have the user provide a tab-delimited dataframe file where the columns are the 5 STITCH model parameters and the rows are the values for those parameters. To make formatting easier, a template file is generated for you, just replace the values and add/remove rows as necessary. See the Imputation section for details on these parameters."]}],[{"l":"Quality Trimming Sequence Data","p":["You can remove adapters and quality trim sequences using:","at least 2 cores/threads available","b/gzipped fastq sequence files","paired-end files","file extension is either .fastq.gz or .fq.gz(do not mix)","forward-reverse is noted as either .1./.2. or.F./.R.(do no mix)","e.g. samplename.F.fq.gz and samplename.R.fq.gz","e.g. samplename.1.fq.gz and samplename.2.fq.gz","or the same but ending with .fastq.gz, but don't mix and match"]},{"l":"Running Options","p":["--dir","--extra-params","--help","--max-length","--snakemake","--threads","-d","-l","-s","-t","-x","20000","4","Additional Hapcut2 parameters, in quotes","Additional Snakemake options, in quotes","argument","default","description","Directory with sequence alignments","folder path","integer","Maximum length to trim sequences down to","no","Number of threads to use","required","short name","Show the module docstring","string","type","yes"]},{"l":"Fastp Workflow","p":["Fastp is an ultra-fast all-in-one adapter remover, deduplicator, and quality trimmer. Harpy uses it to remove adapters, low-quality bases, and trim sequences down to a particular length (default 150bp). Harpy uses the fastp overlap analysis to identify adapters for removal and a sliding window approach (--cut-right) to identify low quality bases. The workflow is quite simple."]}],[{"l":"Mapping Reads onto a Reference Genome","p":["You can map reads onto genome assemblies with Harpy by calling the align module:","at least 4 cores/threads available","a genome assembly in FASTA format","b/gzipped fastq sequence files","paired-end files","file extension is either .fastq.gz or .fq.gz(do not mix)","forward-reverse is noted as either .R1./.R2. or _R1./_R2.(do no mix)","e.g. samplename.R1.fq.gz and samplename.R2.fq.gz","e.g. samplename_R1.fq.gz and samplename_R2.fq.gz","or the same but ending with .fastq.gz, but don't mix and match"]},{"l":"Running Options","p":["--bwa","--dir","--ema-bins","--extra-params","--genome","--help","--snakemake","--threads","-b","-d","-e","-g","-s","-t","-x","4","500","Additional EMA-align/BWA parameters, in quotes","Additional Snakemake options, in quotes","argument","default","description","Directory with sample sequences","file path","folder path","Genome assembly for read mapping","integer","no","Number of barcode bins for EMA","Number of threads to use","required","short name","Show the module docstring","string","toggle","type","Use BWA MEM instead of EMA","yes"]},{"l":"Workflows"},{"l":"EMA","p":["recommended, see bottom of page","leverages the BX barcode information to improve mapping","slower","lots of temporary files","Since EMA does extra things to account for barcode information, the EMA workflow is a bit more complicated under the hood. Reads with barcodes are aligned using EMA and reads without valid barcodes are separately mapped using BWA before merging all the alignments together again. EMA will mark duplicates within alignments, but the BWA alignments need duplicates marked manually using sambamba. Thankfully, you shouldn't need to worry about any of these details."]},{"l":"BWA","p":["ignores barcode information","might be preferred depending on experimental design","faster","no temporary files","The BWA MEM workflow is substantially simpler than the EMA workflow and maps all reads against the reference genome, no muss no fuss. Duplicates are marked at the end using sambamba."]},{"i":"why-ema","l":"Why EMA?","p":["The original haplotag manuscript uses BWA to map reads, but the authors have since then recommended the use of EMA (EMerald Aligner) for most applications. EMA is barcode-aware, meaning it considers sequences with identical barcodes to have originated from the same molecule, and therefore has higher mapping accuracy than using BWA. Here's a comparison from the EMA manuscript: EMA Publication figure 3"]}],[{"l":"Calling Variants","p":["You can call variants with Harpy by calling the variants module:","at least 4 cores/threads available","a genome assembly in FASTA format","alignment files","sample grouping file [optional, see below]"]},{"l":"Running Options","p":["--dir","--extra-params","--genome","--help","--leviathan","--ploidy","--populations","--snakemake","--threads","-d","-g","-l","-p","-s","-t","-x","2","4","Additional mpileup/Leviathan parameters, in quotes","Additional Snakemake options, in quotes","argument","Call variants with Leviathan instead of bcftools","default","description","Directory with sequence alignments","file path","folder path","Genome assembly for variant calling","integer","no","Number of threads to use","Ploidy of samples","required","short name","Show the module docstring","string","Tab-delimited file of sample<tab>group","toggle","type","yes"]},{"l":"sample grouping file","p":["This file is entirely optional and useful if you want variant calling to happen on a per-population level.","takes the format of sample<tab>group","create with harpy init -p samplefolder or manually","if created with harpy init -p, all the samples will be assigned to group 1, so make sure to edit the second column to reflect your data correctly.","the file looks like:"]},{"l":"Workflows"},{"l":"bcftools mpileup","p":["The mpileup and call modules from bcftools(formerly samtools) are used to call variants from alignments. This is a tried-and-true method and one of methods featured in other variant callers, such as that provided in ANGSD, which is why Harpy uses it by default. To speed things along, Harpy will parallelize mpileup to call variants separately on different contigs, then merge everything at the end. This would mean that a more fragmented assembly would probably run faster than a chromosome-scale one, but you're more likely to have fewer variants detected. All intermediate outputs are removed, leaving you only the raw variants file (in .bcf format), the index of that file, and some basic stats about it."]},{"l":"Leviathan","p":["Leviathan is an alternative variant caller that uses linked read barcode information to call structural variants (indels, etc.). Harpy first uses LRez to index the barcodes in the alignments, then it calls variants for individual samples using Leviathan. Due to the nature of Structural Variant (SV) VCF files, there isn't an entirely fool-proof way of combining the variants of all the samples into a single VCF file, therefore the output will be a VCF for every sample."]}],[{"l":"Impute Genotypes using Sequences","p":["You can impute genotypes with Harpy by calling the impute module:","a tab-delimited parameter file","create with harpy impute --init","modify the file with parameters suitable for your study","a variant call format file ( e.g. from mpileup with harpy variants ...)","accepted formats: .vcf, .vcf.gz, .bcf","sequence alignments, in .bam format"]},{"l":"Running Options","p":["--directory","--help","--parameters","--snakemake","--threads","--vcf","-d","-p","-s","-t","-v","4","Additional Snakemake options, in quotes","argument","default","description","Directory with sequence alignments","file path","folder path","integer","no","Number of threads to use","Path to VCF/BCF file","required","short name","Show the module docstring","STITCH parameter file (tab-delimited)","stitch.params","string","type","yes"]},{"l":"Setting Parameters"},{"l":"Parameter file","p":["â‰¥ 1","accepted values","all column names begin with a lowercase character","column name","column order doesn't matter, but all 5 column names must be present","description","Estimated number of generations since founding","header row present with the specific column names below","integer","k","model","nGen","Number of founder haplotypes","Number of instances of the founder haplotypes to average results over","pseudoHaploid, diploid, diploid-inbred","s","tab or comma delimited","text","text/boolean","The STITCH model/method to use","true, false, yes, no (case insensitive)","Typically, one runs STITCH multiple times, exploring how results vary with different model parameters. The solution Harpy uses for this is to have the user provide a tab-delimited dataframe file where the columns are the 5 STITCH model parameters and the rows are the values for those parameters. The parameter file is required and can be created manually or with harpy init -s filename. If created using harpy, the resulting file includes largely meaningless values that you will need to adjust for your study. The parameter must follow a particular format:","useBX","value type","Whether to incorporate beadtag information"]},{"l":"example","p":["This file is tab-delimited, note the column names:","This is the table view of the tab-delimited file, shown here for clarity.","model","useBX","k","s","nGen","pseudoHaploid","TRUE","10","5","50","1","15","100"]},{"l":"Parameters","p":["STITCH uses one of three \"methods\" reflecting different statistical and biological models:","diploid: the best general method with the best statistical properties","run time is proportional to the square of k and so may be slow for large, diverse populations","pseudoHaploid: uses statistical approximations that makes it less accurate than diploid","run time is proportional to k and may be suitable for large, diverse populations","diploid-inbred: assumes all samples are completely inbred and as such uses an underlying haplotype based imputation model","run time is proportional to k","Each model assumes the samples are diploid and all methods output diploid genotypes and probabilities.","This parameter is given as a true/false. Simulations suggest including linked-read information isn't helpful in species with short haploblocks (it might makes things worse). So, it's worth trying both options if you aren't sure about the length of haplotype blocks in your species.","The k parameter is the number of ancestral haplotypes in the model. Larger K allows for more accurate imputation for large samples and coverages, but takes longer and accuracy may suffer with lower coverage. There's value in in trying a few values of k and assess performance using either external validation, or the distribution of quality scores( e.g. mean / median INFO score). The best k gives you the best performance (accuracy, correlation or quality score distribution) within computational constraints, while also ensuring k is not too large given your sequencing coverage ( e.g. try to ensure that each ancestral haplotype gets at least a certain average _X of coverage, like 10X, given your number of samples and average depth).","The s parameter controls the number of sets of ancestral haplotypes used and which final results are averaged over. This may be useful for wild or large populations, like humans. The s value should affect RAM and run time in a near-linearly.","The nGen parameter controls recombination rate between the sequenced samples and the ancestral haplotypes. It's probably fine to set it to \\frac {4 \\times Ne} {k} given some estimate of effective population size {Ne}. If you think your population can be reasonably approximated as having been founded some number of generations ago or reduced to 2 \\times k that many generations ago, use that generation time estimate. STITCH should be fairly robust to misspecifications of this parameter."]},{"l":"STITCH Workflow","p":["STITCH is a genotype imputation software developed for use in the R programming language. It has quite a few model parameters that can be tweaked, but HARPY only focuses on a small handful that have the largest impact on the quality of the results. Imputation is performed on a per-contig (or chromosome) level, so Harpy automatically iterates over the contigs present in the input variant call file. Using the magic of Snakemake, Harpy will automatically iterate over these model parameters."]}],[{"l":"Phasing Haplotypes","p":["You can phase genotypes into haplotypes using:","at least 2 cores/threads available","a vcf/bcf file of genotypes","like the one produce using harpy impute","alignment files in .bam format"]},{"l":"Running Options","p":["--dir","--extra-params","--help","--molecule-distance","--prune-threshold","--snakemake","--threads","--vcf","-d","-m","-p","-s","-t","-v","-x","20000","4","7","Additional Hapcut2 parameters, in quotes","Additional Snakemake options, in quotes","argument","Base-pair distance dilineating separate molecules","default","description","Directory with sequence alignments","file path","folder path","integer","integer (0-100)","no","Number of threads to use","Path to BCF/VCF file","PHRED-scale (%) threshold for pruning low-confidence SNPs","required","short name","Show the module docstring","string","The molecule distance is and pruning thresholds are considered the most impactful parameters for running HapCut2, therefore they are directly configurable from the command. The molecule distance refers to the base-pair distance dilineating separate molecules. Feel free to play around with this number if you do not know the distance, as it's not clear how impactful this can be on the results. The pruning threshold refers to a PHRED-scale value between 0-1 (a percentage) for removing low-confidence SNPs from consideration. With Harpy, you configure this value as an integer between 0-100, which gets converted to a floating point value between 0-1 internally ( i.e.-p 7 is equivalent to 0.07 threshold).","type","yes"]},{"l":"HapCut2 Workflow","p":["Phasing is performed using HapCut2. Most of the tasks cannot be parallelized, but HapCut2 operates on a per-sample basis, so the workflow is parallelized across all of your samples to speed things along."]}]]