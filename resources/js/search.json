[[{"l":"Install Harpy","p":["Harpy is hosted on Bioconda, which means you just need to have either conda or pixi(or docker) on your Unix-like system to install it."]}],[{"l":"Input Format"},{"l":"Read length","p":["Reads must be at least 30 base pairs in length for alignment. By default, the module removes reads <30bp."]},{"l":"Compression","p":["Harpy generally doesn't require the input sequences to be in gzipped/bgzipped format, but it's good practice to compress your reads anyway. Compressed files are expected to end with the extension"]},{"l":"Naming conventions","p":["Unfortunately, there are many different ways of naming FASTQ files, which makes it difficult to accomodate every wacky iteration currently in circulation. While Harpy tries its best to be flexible, there are limitations."]}],[{"i":"#","p":["What you ought to know about linked reads"]},{"l":"Intro to linked-read data","p":["Harpy was originally tailor-made for haplotagging linked read data, but now it works for most linked-read technologies along with non-linked read data. BRL/LRTK/LongRanger are similar pieces of software for Tellseq, stLFR, and 10X linked-read"]},{"l":"Linked reads","p":["Let's start with the most obvious, what are linked reads, which are sometimes called \"synthetic long reads\"? They are short read(e.g. Illumina) data. What makes them different is that they contain an added DNA segment (\"barcode\") that lets us associate"]},{"l":"What do they look like?","p":["Linked-read data is sequence data as you would expect it, encoded in a FASTQ file. The first processing step of linked-read data is demultiplexing to split the raw Illumina-generated batch FASTQ file into samples (if multisample)"]},{"l":"Deconvolution","p":["There are approaches to deconvolve linked read data (or \"deconvolute\", if you're indifferent to the burden of an extra syllable). In this context, deconvolution is finding out which sequences are sharing barcodes by chance rather than because they originated"]},{"l":"Linked-read varieties","p":["There are a handful of linked-read sample preparation methods ( read below), but that's largely an implementation detail. All of those methods are laboratory procedures to take genomic DNA and do the necessary modifications to fragment long DNA molecules, tag the resulting fragments with the same"]},{"l":"Failsafe","p":["Unlike some other fancy well-touted sample preparation methods ( cough cough mate-pair), linked-read data is whole genome sequencing (WGS). What that means is that whether you use the linked-read information or not, the data will always be standard and"]},{"l":"Linked-read library performance","p":["Because linked-read data has an extra dimension, there are some additional metrics that are useful to look at to evaluate how \"good\" the linked-read library turned out. As a baseline for comparison, think of RAD or WGS data and the kinds of metrics you might look at"]},{"l":"Molecule Coverage","p":["Since linked reads are tagger per DNA molecule, we are interested in understanding coverage on a per-molecule basis. By \"molecule\", we are referring to the original DNA molecule from which barcoded sequences originated from."]},{"l":"reads per molecule","p":["It's quite rare that all fragments of a single DNA molecule will get sequenced, so we are interested in getting an average number of reads (sequences) per original DNA molecule. This is done by getting counts of all the reads with the same barcode, as unique barcodes"]},{"l":"percent molecule coverage","p":["Because only a few fragments from a DNA molecule end up getting sequenced, it would be good to understand how much of a molecule is represented in seqences (breadth). It's impossible to get an accurate calculation for this metric because we have no way of knowing"]},{"l":"Singletons","p":["Because linked reads need to be, well, linked, we need to know exactly how many of the sequences actually share barcodes. A barcode that only appears in one paired or unpaired read is a"]},{"l":"Linked Coverage","p":["Because of the \"linked\" component of linked-read data, we have an additional kind of sequence coverage to consider, which is linked coverage. Since linked-read barcodes preserve long-distance information, we can do an alternative kind of"]},{"l":"Linked-read data types","p":["This isn't really the place to go into the nitty-gritty of the different linked-read chemistries(i.e. I don't actually know the nuanced details), but it's worth describing the obvious differences"]},{"l":"Barcode thresholds","p":["By the nature of linked read technologies, there will (almost always) be more DNA fragments than unique barcodes for them. As a result, it's common for barcodes to reappear in sequences. Rather than incorrectly assume that all sequences/alignments with the same barcode"]}],[{"l":"Common Harpy Options"},{"l":"Input Arguments","p":["Each of the main Harpy modules (e.g. or ) follows the format of"]},{"l":"Software Dependencies","p":["Harpy workflows typically require various different pieces of software to run. To keep the Harpy installation small, we include only the bare minimum to invoke Harpy. Everything else (e.g."]},{"l":"Common command-line options","p":["Every Harpy module has a series of configuration parameters. These are arguments you need to input to configure the module to run on your data, such as the directory with the reads/alignments,"]},{"l":"--unlinked","p":["As of version 3.0, Harpy tries to auto-detect the type of data that input FASTQ or BAM files may be: haplotagging, stlfr, tellseq, or none. The formats for these data and how their barcodes look are described"]},{"l":"--contigs","p":["Some of the workflows (like ) plot per-contig information in their reports. By default, Harpy will plot up to 30 of the largest contigs. If you are only interested in a specific set of contigs, then you can use"]},{"l":"example","p":["You could call and specify 20 threads with no output to console:"]},{"l":"The workflow folder","p":["When you run one of the main Harpy modules, the output directory will contain a workflow folder. This folder is both necessary for the module to run and is very useful to understand what the module did, be it for your own"]}],[{"i":"#","p":["A gentle introduction to the wild world of filtering SNPs"]},{"l":"Filtering SNPs"},{"l":"TL;DR","p":["The discussion around filtering SNPs and indels is massive and many researchers go about it differently, each very opinionated as to why their method is the best. As a starting point, have a look at how the authors of"]},{"l":"genotype quality (QUAL)","p":["You will obviously want higher quality genotype calls to remove false positives. The HTSlib guide suggests at least 50(e.g. -i 'QUAL=50'), but we typically filter much higher at"]},{"l":"read depth (DP)","p":["Variant sites with too few reads backing up the genotype might be false positives, although this may not hold true for very low-coverage data. Conversely, a maximum cut off is important because sites with very high read depths (relative to the distribution of read depth)"]},{"l":"minor allele frequency (MAF)","p":["It's usually advisable to set a minor allele frequency threshold with which to remove sites below that threshold. The reasoning is that if a MAF is too low, it might be because of incorrectly called genotypes in a very small handful of individuals (e.g. one or two)."]},{"l":"missing data (F_MISSING)","p":["Missing data is, frankly, not terribly useful. The amount of missing data you're willing to tolerate will depend on your study, but it's common to remove sites with >20% missing data (e.g."]}],[{"i":"#","p":["A realistic workflow to simulate variants"]},{"l":"Simulating variants"},{"l":"TL;DR","p":["You may want to (and are encouraged to) simulate data before investing in the costs associated with linked-read sample preparation and subsequent sequencing. Harpy provides both a variant and linked-read simulators and this tutorial serves to"]},{"l":"1. Add random inversions","p":["First, we will need to simulate some inversions and set a --heterozygosity value >0 to get a diploid genome as the output. If you wanted to manually create inversions in specific areas or with specific lengths, this would be a good starting point too since"]},{"l":"2. Add snps and indels","p":["Let's say we wanted to simulate SNPs and indels like so:"]},{"l":"3. Simulate \"known\" snps and indels onto the diploid genome with inversions","p":["We will run Harpy twice, once for each haplotype, using the corresponding VCFs from Step 2:"]},{"l":"5. Simulating linked-reads","p":["Now that you have heterozygous haplotypes created from your starting genome, you can simulate linked-reads from it using harpy simulate linkedreads. A simple implementation of that could look like:"]}],[{"i":"#","p":["Sorting data by linked-read barcode"]},{"l":"Sort data by barcode"},{"l":"TL;DR","p":["You would think sorting data would be a no-brainer, and in most cases it is. You can use seqtk or seqkit to sort FASTQ/A files by their IDs, samtools to sort SAM/BAM/CRAM files by name or coordinates. However, in the world of linked-read"]},{"l":"Sorting Alignments","p":["Let's start with BAM (or SAM/CRAM) files because the process is much simpler. Since the linked-read barcode is stored in a BX:Z tag (or less often as BC:Z:), we can use a little feature of"]},{"l":"Sorting FASTQ","p":["Sorting FASTQ files by barcode is trickier, only because there aren't (to our knowledge!) any existing convenience methods to do it. Like any bioinformatics puzzle, you could probably solve it with a sophisticated AWK command, but HTSlib tools are so much more"]},{"l":"1. convert FASTQ to SAM","p":["Yep, we're solving our problem by doing a simple file conversion to SAM/BAM. That's the easiest way to do it, surprisingly. FASTQ files can be converted to unmapped BAM files using"]},{"l":"2. sort the SAM by barcode","p":["Exactly like shown above to sort a SAM/BAM file with samtools sort, we're going to do the same on the unmapped SAM file we just created:"]},{"l":"3. convert SAM back to FASTQ","p":["Now that the data have been sorted, we need to convert it back into forward and reverse FASTQ files using samtools fastq. The -T * argument once again preserves all the tags between file formats. The"]},{"l":"as a single pipe","p":["Rather than splitting out these three processess, you can stream/pipe them in a single workflow:"]}],[{"i":"#","p":["How to use Harpy for plain-regular WGS data"]},{"l":"Harpy for (non linked-read) WGS data","p":["As of Harpy v3, the program will auto-detect that your input FASTQ or BAM files are not linked-read data. This can also be forced with --unlinked/ -U."]}],[{"i":"#","p":["Deciding between using Conda or Containers"]},{"l":"Choosing a software runtime method"},{"l":"TL;DR","p":["There are two ways you can run Harpy, using a container with the necessary software environments in it (--container), or with local conda environments(the default). If software development and containerization"]},{"l":"What Harpy Provides","p":["An conda-based installation of Harpy provides only the minimal set of programs Harpy needs to begin a workflow. These include: python, snakemake-minimal, the htslib programs (e.g."]},{"l":"How Harpy Provides the Other Stuff","p":["Instead of a monolithic Harpy environment, which would be impossible with the current software dependencies, there are a handful of defined conda environment recipes that Harpy"]},{"l":"Harpy and Containers","p":["The Harpy team manages a container on Dockerhub called, you guessed it, Harpy, that is synchronously versioned with the Harpy software. In other words, if you're using Harpy v1.4, it will use the container version v1.4. The"]},{"l":"What's the Catch?","p":["While local conda enviroments at runtime or containers might seem like foolproof approaches, there are drawbacks."]},{"l":"Conda Caveats:"},{"l":"⚠️ Conda Caveat 1: Inconsistent","p":["Despite our and conda's best efforts, sometimes programs just don't install correctly on some systems due to unexpected system (or conda) configurations. This results in frustrating errors where jobs fail because software that is"]},{"l":"\uD83D\uDCA3 Conda Caveat 2: Troubleshooting","p":["To manually troubleshoot many of the tasks within Harpy workflows, you may need to jump into one of the local conda environments in .environments. That itself isn't terrible, but it's an extra step because you will"]},{"l":"Container Caveats"},{"l":"\uD83D\uDEA5 Container Caveat 1: Speed","p":["The overhead of Snakemake creating a container instance for a job, then cleaning it up after the job is done is not trivial and can negatively impact runtime."]},{"l":"\uD83D\uDCA3 Container Caveat 2: Troubleshooting","p":["The command Snakemake secretly invokes to run a job in a container is quite lengthy. In most cases that shouldn't matter to you, but when something eventually goes wrong and you need to troubleshoot, it's harder"]}],[{"i":"#","p":["Why pool samples for SV calling and when to do it"]},{"l":"Pooling samples for SV calling"},{"l":"TL;DR","p":["One of the cool benefits of linked-read data is the fact that you can call structural variants with it. Depending on the depth of your data, you may want (or need) to pool samples together. This"]},{"l":"Sample depth"},{"l":"Depth, explained","p":["In bioinformatics, the terms \"coverage\" and \"depth\" and often used interchangeably, which is incorrect and leads to confusion. Coverage refers to the proportion of a genome that is sequenced, and"]},{"l":"Depth, in context","p":["Historically, one would have wanted to sequence fewer individuals at higher depth to get confident genotype calls, rather than sequence more individuals at lower depth. Recent advances in bioinformatics have enabled low-coverage whole genome sequencing"]},{"l":"The problem","p":["It's recommended to have at least 10X-12X depth to get decent structural variant calls(definitely read that in a paper that I would like to link here, but I can't seem to find it). If your data already has a minimum of 10X for each individual, great! Feel free to use"]},{"l":"The solution","p":["One way to get your low-coverage (low depth) data and still call structural variants is to pool samples together, which would effectively boost the depth. By doing this, you will"]},{"l":"Pooling considerations","p":["If pooling samples, you must pool them sensibly and with a biological context to do so. In other words, you don't just pool random samples together to inflate depth. Since haplotag data is just whole genome sequence data plus a little extra information, you should"]}],[{"l":"Developing Harpy","p":["Harpy is an open source program written using a combination of BASH, R, RMarkdown/Quarto, Python, and Snakemake. This page provides information on Harpy's development and how to contribute to it, if you were inclined to do so."]},{"l":"Installing dev version","p":["As of v1.15, we provide two scripts that automate a development installation(we use them ourselves!). First, you'll need to clone the Harpy git repository:"]},{"l":"Harpy's components"},{"l":"source code","p":["Harpy runs in two stages:"]},{"l":"bioconda recipe","p":["For the ease of installation for end-users, Harpy has a recipe and build script in Bioconda, which makes it available for download and installation. A copy of the recipe is also"]},{"l":"The Harpy repository"},{"l":"repo structure","p":["Harpy exists as a Git repository and has 3 standard branches that are used in specific ways during development. Git is a popular version control system and discussing its use is out of the scope of this documentation, however there is no"]},{"l":"development workflow","p":["The dev workflow is reasonably standard:"]},{"l":"containerization","p":["As of Harpy v1.0, the software dependencies that the Snakemake workflows use are pre-configured as a Docker image that is uploaded to Dockerhub. Updating or editing this container can be done automatically or manually."]},{"l":"automatically","p":["The testing GitHub Action will automatically create a Dockerfile with (a hidden harpy command) and build a new Docker container, then upload it to dockerhub with the latest tag. This process is triggered on"]},{"l":"manually","p":["The dockerfile for that container is created by using a hidden harpy command"]},{"l":"Automations"},{"l":"testing","p":["CI ( C ontinuous I ntegration) is a term describing automated actions that do things to/with your code and are triggered by how you interact with a repository. Harpy has a series of GitHub Actions triggered by interactions with the"]},{"l":"releases","p":["There is an automation that gets triggered every time Harpy is tagged with the new version. It strips out the unnecessary files and will upload a cleaned tarball to the new release (reducing filesize by orders of magnitude). The automation will also"]}],[],[{"l":"Running on an HPC Cluster","p":["You'll be working with genomic data, so it's very likely you'll try to run Harpy on a high-performance computing (HPC) cluster at some point. Doing so will usually require you to log into a head node and submit jobs that some kind of scheduler (e.g. SLURM, HTCondor)"]},{"l":"Snakemake HPC execution","p":["Snakemake has executor plugins for various cluster job managers (like SLURM) and storage plugins (like fs or s3) to automate workflow execution and file transfer in HPC contexts. In order"]},{"l":"HPC features in Harpy","p":["Harpy provides this Snakemake-driven HPC support with the --hpc option available to most workflows. This option requires a path to the configuration file with the HPC configuration yaml. In practice, that would look something like:"]},{"l":"Configuration templates","p":["This configuration stuff is a lot of congitive burden in addition to just trying to process your data, so you can use to create skeleton configurations for various supported cluster managers and fill in the information you need:"]}],[{"l":"Snakamake Things"},{"l":"Workflow logs","p":["Barring a few exceptions, most of Harpy's options are Snakemake workflows. This means we are all at the mercy of how Snakemake operates, which includes the .snakemake/ folder in your project directory. That folder contains"]},{"l":"Adding Snakemake Parameters","p":["Harpy relies on Snakemake under the hood to handle file and job dependencies. Most of these details have been abstracted away from the end-user, but most Harpy modules have an optional flag"]},{"l":"Common use cases","p":["You likely wont need to invoke --snakemake very often, if ever. However, here examples of some possible use cases for this parameter."]}],[{"l":"Software used in Harpy","p":["Harpy is the sum of its parts, and out of tremendous respect for the developers involved in the included software, we would like to highlight the tools directly involved in Harpy's many moving pieces."]},{"l":"Standalone Software"},{"l":"Software Packages"}],[{"l":"Utilities","p":["Harpy is the sum of its parts and some of those parts are stand-alone scripts used by the workflows that are accessible from within the Harpy conda environment. This page serves to document those scripts, since using them outside of a workflow"]},{"l":"assign_mi","p":["Assign an MI:i(Molecular Identifier) tag to each barcoded record based on a molecular distance cutoff. Input file must be coordinate sorted. This is similar to deconvolve_alignments"]},{"l":"bx_stats","p":["Calculates various linked-read molecule metrics from the (coordinate-sorted) input alignment file. Metrics include (per molecule):"]},{"l":"bx_to_end","p":["Parses the records of a FASTQ or BAM file and moves the BX:Z tag, if present, to the end of the record, which makes the data play nice with LRez/LEVIATHAN. During alignment, Harpy will automatically move the"]},{"l":"check_bam","p":["Parses an aligment file to check:"]},{"l":"check_fastq","p":["Parses a FASTQ file to check if any sequences don't conform to the SAM spec, whether BX:Z: is the last tag in the record, and the counts of:"]},{"l":"concatenate_bam","p":["Concatenate records from haplotagged SAM/BAM files while making sure MI tags remain unique for every sample. This is a means of accomplishing the same as samtools cat, except all"]},{"l":"count_bx","p":["Parses a FASTQ file to count:"]},{"l":"deconvolve_alignments","p":["Deconvolve BX-tagged barcodes and assign an MI(Molecular Identifier) tag to each barcoded record based on a molecular distance cutoff. Input file must be coordinate sorted. This is similar to"]},{"l":"depth_windows","p":["Reads the output of samtools depth -a from stdin and calculates means within windows of a given windowsize."]},{"l":"haplotag_acbd","p":["Generates the BC_{ABCD}.txt files necessary to demultiplex Gen I haplotag barcodes into the specified output_directory."]},{"l":"infer_sv","p":["Create column in NAIBR bedpe output inferring the SV type from the orientation. Removes variants with FAIL flags and you can use the optional -f(--fail) argument to output FAIL variants to a separate file."]},{"l":"inline_to_haplotag","p":["Converts inline nucleotide barcodes in reads to haplotag linked reads with barcodes in BX:Z and OX:Z header tags. The barcodes.txt file can be gzipped and must be in the form of nucleotide-barcode"]},{"l":"make_windows","p":["Create a BED file of fixed intervals (-w, -- window) from a FASTA or fai file (the kind generated with samtools faidx). Nearly identical to bedtools makewindows, except the intervals are nonoverlapping. The"]},{"l":"molecule_coverage","p":["Using the statsfile generated by bx_stats from Harpy, will calculate \"molecular coverage\" across the genome. Molecular coverage is the \"effective\" alignment coverage if you treat a molecule inferred from linked-read data as"]},{"l":"parse_phaseblocks","p":["Parse a phase block file from HapCut2 to pull out summary information"]},{"l":"rename_bam","p":["Rename a sam/bam file and modify the @RG tag of the alignment file to reflect the change for both ID and SM. This process creates a new file new_name.bam and you may use -d to delete the original file. Requires"]},{"l":"separate_singletons","p":["Isolate singleton and non-singleton linked-read BAM records into separate files. Singletons refers to barcodes that have only one unpaired or paired read, meaning the barcode doesn't"]},{"l":"separate_validbx","p":["Split a BAM file with BX tags by tag validity"]},{"l":"standardize_barcodes_sam","p":["Parse a SAM/BAM file to identify the linked-read barcode (haplotagging, stlfr, tellseq, 10x) and move it to the BX:Z tag if it's not already there. After, add a VX:i:0 if the barcode is"]}],[{"l":"Basic Diagnostics","p":["Lots of stuff can go wrong during an analysis. The intent of this page is to guide you through navigating the inevitable errors associated with doing bioinformatics."]},{"l":"Logging Streams","p":["In shell programs, output streams are routed through standard out ( stdout) and standard error ( stderr) streams. When not using --quiet, Harpy will output some overview text, have an updating progress bar that disappears"]},{"l":"Troubleshooting Harpy","p":["Harpy has two steps: first it performs checks and validations, then it runs Snakemake."]},{"l":"checks and validations","p":["First, Harpy takes your command-line inputs and checks/validates the input files and parameters. If your parameters are not the correct type (e.g. a number where there should be a file), the"]},{"l":"snakemake validations","p":["Once all the file validations pass, Harpy passes the baton over to Snakemake. Snakemake builds a workflow graph of the rules and performs its own checks. If you get an error before the workflow starts processing any data (there"]},{"l":"error during a workflow","p":["Sometimes something goes wrong with one of the steps in a workflow. If/when that happens, Harpy will print the offending step and all the information Snakemake has regarding the failure. If the step had a log file, it will"]},{"l":"harpy diagnose","p":["The harpy diagnose command will run dry-run Snakemake for a workflow with the --dry-run --debug-dag options and print output to try to identify why Snakemake might be stalling for a workflow."]},{"l":"Common Issues"},{"l":"imputation or phasing failure","p":["If you use bamutils clipOverlap on alignments that are used for the or modules, they will cause both programs to error. We don't know why, but they do."]},{"l":"SAM name and ID mismatch","p":["Aligning a sample to a genome via Harpy will insert the sample name (based on the file name) into the alignment header (the @RG ID:name SM:name tag). It likewise expects, through various steps,"]}],[{"l":"Install / Conda"},{"l":"Installation issues","p":["Conda is an awesome package manager, but was slow and used a ton of memory as dependencies increased. Recent ( 23.10+) versions of Conda now use the libmamba solver, the super-fast and super-lightweight solver from Mamba. If you're experiencing"]},{"l":"strict channel priorities","p":["Sometimes conda channel priorities may give you grief, be it when trying to install Harpy, or Harpy trying to install a particular workflow's dependencies. Although Conda recommends setting channel priorities to"]},{"l":"Conda issues","p":["Unless using --container, Harpy leans on Snakemake to install small conda environments necessary to accomplish the tasks in workflows. The most common issue we tend to see is errors during this process."]},{"l":"installation into base","p":["It's tempting to install software into the base environment conda/mamba ships with, but that behavior is discouraged (by Conda itself, not just us). Modificatons to base can mess with all sorts of things in other environments ("]},{"l":"Install dependencies manually"},{"l":"conda is forbidden","p":["Some HPC configurations prohibit the use of conda. If that's preventing you from installing Harpy, then you can try to install it as a python package using pip. You will still need to manually install the"]},{"l":"nodes do not have internet access","p":["A not-uncommon HPC setup is to have the login node connected to the internet, but the worker nodes have no internet access. This obviously creates a problem if Snakemake needs to download software dependencies (conda or container). To"]},{"l":"deps conda","p":["Create the conda environments required by Harpy's workflows (e.g. phase). Only useful for specific HPC configurations where worker nodes do not have internet access to let snakemake install conda packages itself."]},{"l":"deps container","p":["Manually pull the Harpy dependency container from dockerhub and convert it into an Apptainer .sif file. Only useful for specific HPC configurations where worker nodes do not have internet access to let snakemake download the container itself."]}],[{"l":"Resume","p":["When calling a workflow (e.g. ), Harpy performs various file checks and validations, sets up the Snakemake command, output folder(s), etc. In the event you want to continue a failed or manually terminated workflow without overwriting the workflow files (e.g."]},{"l":"arguments","p":["The DIRECTORY is the output directory of a previous harpy-invoked workflow, which must have the workflow/config.yaml file. For example, if you previously ran harpy align bwa -o align-bwa ..."]}],[{"l":"View","p":["This convenience command lets you view relevant details within a Harpy workflow directory without having to fish around for the right files."]},{"l":"arguments"},{"l":"modes"},{"l":"environments","p":["view environments is an exception in that it does not require a DIRECTORY argument. To use environments, you can run it without arguments or give it a SOFTWARE argument to only print environments where"]}],[{"i":"#","p":["Align haplotagged sequences"]},{"l":"Align Sequences to a Genome","p":["After your sequences (in FASTQ format) have been checked for quality, you will need to align them to a reference genome before you can call variants. Harpy offers several aligners for this purpose:"]},{"l":"Non linked-read WGS data","p":["Starting with Harpy v2.x, the --lr-type none option (--ignore-bx toggle in versions <2.7) lets you skip the workflow routines that do things specific to linked reads, meaning you can comfortably use"]}],[{"i":"#","p":["Align haplotagged sequences with BWA MEM"]},{"l":"Map Reads onto a genome with BWA MEM","p":["Once sequences have been trimmed and passed through other QC filters, they will need to be aligned to a reference genome. This module within Harpy expects filtered reads as input,"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Output format","p":["Regardless of the input linked-read format, the align workflows will standardize the output alignment records such that the barcode is contained in the BX:Z tag and barcode validation is in the"]},{"l":"Molecule distance","p":["The --molecule-distance option is used during the BWA alignment workflow to assign alignments a unique Molecular Identifier MI:i tag based on their linked-read barcode and the"]},{"l":"Quality filtering","p":["The --min-quality argument filters out alignments below a given MQ threshold. The default, 30, keeps alignments that are at least 99.9% likely correctly mapped. Set this value to"]},{"l":"Marking PCR duplicates","p":["Harpy uses samtools markdup to mark putative PCR duplicates by using both the BX tag as a UMI (unique molecule identified) for more accurate duplicate detection. The read name is also parsed to determine if the sequencing platform was HiSeq/NovaSeq to distinguish between"]},{"l":"BWA workflow"}],[{"i":"#","p":["Align haplotagged sequences with strobealign"]},{"l":"Map Reads onto a genome with strobealign","p":["Once sequences have been trimmed and passed through other QC filters, they will need to be aligned to a reference genome. This module within Harpy expects filtered reads as input,"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Molecule distance","p":["The --molecule-distance option is used during the BWA alignment workflow to assign alignments a unique Molecular Identifier MI:i tag based on their haplotag barcode and the distance threshold"]},{"l":"Quality filtering","p":["The --min-quality argument filters out alignments below a given MQ threshold. The default, 30, keeps alignments that are at least 99.9% likely correctly mapped. Set this value to"]},{"l":"Marking PCR duplicates","p":["Harpy uses samtools markdup to mark putative PCR duplicates by using both the BX tag as a UMI (unique molecule identified) for more accurate duplicate detection. The read name is also parsed to determine if the sequencing platform was HiSeq/NovaSeq to distinguish between"]},{"l":"Strobealign workflow"}],[{"i":"#","p":["Create a genome assembly from linked reads"]},{"l":"Create a Genome Assembly","p":["If you have single-sample data, you might be interested in a genome assembly. Unlike metagenome assemblies, a classic genome assembly assumes there is exactly one genome present in your sequences and will try to"]},{"l":"Running Options","p":["In addition to the , the module is configured using the command-line arguments below. Since the assembly process consists of several distinct phases, the descriptions are provided with an extra badge to reflect which part of the assembly process they correspond to."]},{"l":"Deconvolved Inputs","p":["For linked-read assemblies, the barcodes need to be deconvolved in the sequence data, meaning that barcodes that are shared by reads that originate from different molecules need to have unique barcode"]},{"l":"Assembly Workflow"}],[{"i":"#","p":["Convert between linked-read data formats"]},{"l":"Convert between data formats","p":["Regrettably, the bright minds who developed various linked-read technologies cannot seem to agree on a unified data format. That's annoying at best and hinders the field of linked-read analysis at worst, as there are pieces of very clever software"]},{"l":"Convert FASTQ formats","p":["In the event you need your linked-read data converted into a different linked-read format to use outside of Harpy: don't worry, we got you covered. We might disagree on the fragmented format landscape, but that doesn't mean you"]},{"l":"Conversion targets"},{"l":"Running Options"},{"l":"Standardize","p":["In the effort of making it painless to have your data in the preferred standard format, Harpy provides convert standardize-* to quickly standardize FASTQ and BAM files. By default, standardization just moves the barcode (wherever it may be)"]},{"l":"BAM","p":["If barcodes are present in the sequence name (stlfr, tellseq), this method moves the barcode to the BX:Z tag of the alignment, maintaining the same barcode style by default (auto-detected). If moved to or already in a"]},{"i":"running-options-1","l":"Running Options"},{"l":"FASTQ","p":["This conversion moves the barcode to the BX:Z tag in fastq records, maintaining the same barcode type by default (auto-detected). See this section for the location and format expectations for different linked-read technologies."]},{"i":"running-options-2","l":"Running Options"}],[{"i":"#","p":["Resolve barcodes shared by different molecules"]},{"l":"Resolve barcodes shared by different molecules","p":["Running is optional. In the alignment workflows (), Harpy already uses a distance-based approach to deconvolve barcodes and assign MI tags (Molecular Identifier). This workflow uses a reference-free method,"]},{"l":"Running Options"},{"l":"Resulting Barcodes","p":["After deconvolution, some barcodes may have a hyphenated suffix like -1 or -2(e.g. A01C33B41D93-1). This is how deconvolution methods create unique variants of barcodes to denote that identical barcodes"]},{"l":"Harpy Deconvolution Nuances","p":["Some of the downstream linked-read tools Harpy uses expect linked read barcodes to either look like the 16-base 10X variety or a standard haplotag (AxxCxxBxxDxx). Their pattern-matching would not recognize barcodes deconvoluted with"]}],[{"i":"#","p":["Downsample data by barcode"]},{"l":"Downsample data by barcode","p":["While downsampling (subsampling) FASTQ and BAM files is relatively simple with tools such as awk, samtools, seqtk, seqkit, etc., allows you to downsample a BAM file (or paired-end FASTQ)"]},{"l":"Running Options","p":["In addition to the , the module is configured using the command-line arguments below."]},{"l":"invalid barcodes","p":["The --invalid options determines what proportion of invalid barcodes appear in the barcode pool that gets subsampled. The --invalid proportion doesn't necessarily reflect how many"]},{"l":"Downsample Workflow"}],[{"i":"#","p":["Demultiplex raw sequences into haplotag barcoded samples"]},{"l":"Demultiplex Raw Sequences","p":["When pooling samples and sequencing them in parallel on an Illumina sequencer, you will be given large multiplexed FASTQ files in return. These files contain sequences for all of your samples and need to be demultiplexed using barcodes to"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Keeping Unknown Samples","p":["It's not uncommon that some sequences cannot be demultiplexed due to sequencing errors at the ID location. Use --keep-unknown-samples/-u to have Harpy still separate those reads from the original multiplex. Those reads will be labelled"]},{"l":"Keeping Unknown Barcodes","p":["It's likewise not uncommon that sequencing errors make it so that the sequences don't match the list of known barcode segments. Use--keep-unknown-barcodes/-b to have Harpy separate those reads out from the original multiplex as"]},{"l":"Keep QX and RX Tags","p":["Using --qx-rx, you can opt-in to retain the QX:Z(barcode PHRED scores) and RX:Z(nucleotide barcode) tags in the sequence headers. These tags aren't used by any subsequent analyses, but may be useful for your own diagnostics."]},{"l":"Haplotagging Types"},{"l":"Meier2021 Demultiplex Workflow"},{"l":"Now using dmox!","p":["Harpy v2 introduced a new demultiplexer under the hood called dmox, which is singificantly faster, lighter on memory, and has better maintenance than the previous solution. Iago Bonnici"]}],[{"i":"#","p":["Impute genotypes for haplotagged data with Harpy"]},{"l":"Impute Genotypes using Sequences","p":["After variants have been called, you may want to impute missing genotypes to get the most from your data. Harpy uses STITCH to impute genotypes, a haplotype-based method that is linked-read aware. Imputing genotypes requires a variant call file"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Impute a specific region","p":["Use --region to only impute a specific genomic region, given as contig:start-end-buffer, otherwise all contigs will be imputed. The buffer is an integer for how much before and after"]},{"l":"Extra STITCH parameters","p":["You may add additional parameters to STITCH by way of the--extra-params(or -x) option. Harpy uses the STITCH.R command-line tool, which requires arguments to be in the form --argument=value"]},{"l":"Prioritize the vcf file","p":["Sometimes you want to run imputation on all the samples present in the INPUTS, but other times you may want to only impute the samples present in the VCF file. By default, Harpy assumes you want to use all the samples"]},{"l":"Parameter file","p":["Typically, one runs STITCH multiple times, exploring how results vary with different model parameters (explained in next section). The solution Harpy uses for this is to have the user"]},{"l":"STITCH Parameters"},{"l":"Imputation Workflow"}],[{"i":"#","p":["Create a metagenome assembly from linked reads"]},{"l":"Create a Metagenome Assembly","p":["If you have mixed-sample data, you might be interested in a metagenome assembly, also known as a metassembly. Unlike a single-sample assembly, a metassembly assumes there are multiple genomes present in your sequences and will try to"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Deconvolved Inputs","p":["For linked-read assemblies, the barcodes need to be deconvolved in the sequence data, meaning that barcodes that are shared by reads that originate from different molecules need to have unique barcode"]},{"l":"Metassembly Workflow"}],[{"i":"#","p":["Generate extra files for analysis with Harpy"]},{"l":"Template","p":["Creating template/boilerplate files has been consolidated into harpy template. These commands will generate a specific file and write to stdout."]},{"l":"groupings","p":["Creates a sample grouping file for variant calling"]},{"l":"arguments","p":["This optional file is useful if you want SNP variant calling to happen on a per-population level via or on samples pooled-as-populations via ."]},{"l":"hpc-*","p":["Create template configurations for HPC cluster job submission systems (e.g. SLURM, HTConder) that can be provided to the --hpc option for workflows. You will likely also need to install the appropriate"]},{"l":"impute","p":["Create a template parameter file for the module. The file is formatted correctly and serves as a starting point for using parameters that make sense for your study. Typically, one runs STITCH multiple times, exploring how results vary with"]}],[{"i":"#","p":["Phase haplotypes for haplotagged data with Harpy"]},{"l":"Phase SNPs into Haplotypes","p":["You may want to phase your genotypes into haplotypes, as haplotypes tend to be more informative than unphased genotypes (higher polymorphism, captures relationship between genotypes). Phasing"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Prioritize the vcf file","p":["Sometimes you want to run imputation on all the samples present in the INPUTS, but other times you may want to only impute the samples present in the --vcf file. By default, Harpy assumes you want to use all the samples"]},{"l":"Molecule distance","p":["The molecule distance refers to the base-pair distance dilineating separate molecules. In other words, when two alignments on a single contig share the same barcode, how far away from each other are we willing to say they were and still consider them having"]},{"l":"Pruning threshold","p":["The pruning threshold refers to a PHRED-scale value between 0-1 (a percentage) for removing low-confidence SNPs from consideration. With Harpy, you configure this value as an integer"]},{"l":"Phasing Workflow"}],[{"i":"#","p":["Run file format checks on haplotagged FASTQ/BAM files"]},{"l":"Validation checks for input files","p":["Harpy does a lot of stuff with a lot of software and each of these programs expect the incoming data to follow particular formats (plural, unfortunately). These formatting opinions/specifics are at the mercy of the original developers and while there are times when Harpy can (and does)"]},{"l":"When to run"},{"l":"Running Options","p":["In addition to the , the and modules are configured using only command-line input arguments:"]},{"l":"Workflow"}],[{"i":"#","p":["Quality trim haplotagged sequences with Harpy"]},{"l":"Quality Trim Sequences","p":["Raw sequences are not suitable for downstream analyses. They have sequencing adapters, index sequences, regions of poor quality, etc. The first step of any genetic sequence analyses is to remove these adapters and trim poor quality data. You can remove adapters,"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Deduplicate reads","p":["You can opt-in to have fastp deduplicate optical (PCR) duplicates. It's generally not recommended to perform deduplication during quality-checking, as the workflows use the linked-read barcode to more accurately tag reads as duplicates."]},{"l":"Trim adapters","p":["You can opt-in to find and remove adapter content in sequences."]},{"l":"QC Workflow"}],[{"i":"#","p":["Simulate genomic data"]},{"l":"Simulate Genomic Data","p":["You may be interested in benchmarking variant detection or maybe just trying out haplotagging data without any financial commitment-- that's where simulations come in handy."]},{"l":"Simulate Genomic Variants","p":["Harpy lets you simulate genomic variants via for different variant types such as single nucleotide polymorphisms (SNP), indels, inversions, copy number variants (CNV), and translocations. All you need is to provide a genome to simulate"]},{"l":"Simulate Haplotag Linked-Reads","p":["You can also simulate haplotag-style linked reads from an existing genome using . Harpy incorporates LRSIM to generate linked reads from a diploid genomic. If you only have a haploid genome, then you can create a diploid genome by simulating variants into it with"]}],[{"i":"#","p":["Simulate linked reads from a genome"]},{"l":"Simulate Linked Reads","p":["Simulate linked reads from a genome"]},{"l":"Running Options"},{"l":"Barcodes"},{"l":"Randomly generate","p":["Mimick lets you put in length and count parameters, which it will use to randomly generate barcodes. The format is length,count(no spaces), where length is the base-pair length the barcodes should be and"]},{"l":"Specific barcodes","p":["Alternatively, if you have a set of barcodes you absolutely want to use, just put the filename as the first positional argument. In practice, this would look something like:"]},{"l":"FASTA file(s)","p":["You will need at least 1 fasta file as input, which goes at the very end of the command. It's assumed that each fasta file is a different haplotype of the same genome."]}],[{"i":"#","p":["Simulate snps, indels, inversions, cnv, translocations"]},{"l":"Simulate Genomic Variants","p":["Simulate snps, indels, inversions, cnv, translocations"]},{"l":"Modules","p":["There are 4 submodules with very obvious names:"]},{"l":"Running Options","p":["While there are serveral differences between individual workflow options, each has available all the like other Harpy modules. Each requires and input genome at the end of the command line, and each requires either a"]},{"l":"Simulate known variants","p":["Rather than simulating random variants, you can use a VCF file as input to any of the workflows to have simuG simulate the variants (of that type) from the VCF file. This becomes particularly"]},{"l":"Heterozygosity","p":["Each workflow has a --heterozygosity parameter where you can specify the heterozygosity of the simulated variants, which creates two new VCF files ({prefix}.hap1.vcf,{prefix}.hap2.vcf"]},{"l":"Variant Simulation Workflow"}],[{"i":"#","p":["Call SNPs and small indels"]},{"l":"Call SNPs and small indels","p":["After reads have been aligned, e.g., with , you can use those alignment files(.bam) to call variants in your data. Harpy can call SNPs and small indels using bcftools mpileup or with"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"ploidy","p":["If you are calling haploid or diploid samples, using either mpileup or freebayes will be comparable. However, if you need to call SNPs in polyploids (ploidy >2), then you will need to use"]},{"l":"regions","p":["The --regions(-r) option lets you specify the genomic regions you want to call variants on. Keep in mind that mpileup uses 1-based positions for genomic intervals, whereas freebayes"]},{"l":"populations","p":["Grouping samples changes the way the variant callers computes certain statistics when calling variants. If you have reason to believe there is a biologically meaningful grouping scheme to your samples, then you should include"]},{"l":"SNP calling workflow"}],[{"i":"#","p":["Find structural variants"]},{"l":"Find structural variants","p":["The module identifies single nucleotide polymorphisms (SNP) and small indels, but you may want to (and should!) leverage the linked-read data to identify larger structural variants (SV) like large deletions, duplications, and"]},{"l":"LEVIATHAN","p":["LEVIATHAN relies on split-read information in the sequence alignments to call variants. It requires less preprocessing work to get it up and running, so it's a great place to start."]},{"l":"NAIBR","p":["While our testing shows that NAIBR tends to find known inversions that LEVIATHAN misses, the program requires haplotype phased bam files as input. That means the alignments have a"]}],[{"i":"#","p":["Call structural variants using Leviathan"]},{"l":"Call Structural Variants using LEVIATHAN","p":["After reads have been aligned, e.g. with , you can use those alignment files(.bam) to call structural variants in your data using LEVIATHAN. To make sure your data will work seemlessly with LEVIATHAN, the alignments in the"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Duplicates","p":["If it feels like Leviathan is unneccesarily splitting larger variants into smaller overlapping or adjacent ones, this parameter should help limit that from occuring."]},{"l":"Barcode sharing thresholds","p":["The default Leviathan setting of 99 for these three parameters can be too strict and omit variants you are expecting to find (e.g. certain large inversions). If you suspect the program"]},{"l":"Single-sample variant calling","p":["When not using a population grouping file via --populations, variants will be called per-sample. Due to the nature of structural variant VCF files, there isn't an entirely fool-proof way"]},{"l":"Pooled-sample variant calling","p":["With the inclusion of a population grouping file via --populations, Harpy will merge the bam files of all samples within a population and call variants on these alignment pools. Preliminary work shows that this way identifies more variants and with fewer false"]},{"l":"LEVIATHAN workflow"}],[{"i":"#","p":["Call structural variants using NAIBR (plus)"]},{"l":"Call Structural Variants using NAIBR","p":["After reads have been aligned, e.g. with , you can use those alignment files(.bam) to call structural variants in your data using NAIBR. While our testing shows that NAIBR tends to find known inversions that LEVIATHAN misses, the program requires haplotype"]},{"l":"Running Options","p":["In addition to the , the module is configured using these command-line arguments:"]},{"l":"Molecule distance","p":["The --molecule-distance option is used to let the program determine how far apart alignments on a contig with the same barcode can be from each other and still considered as originating from the same DNA molecule. See"]},{"l":"Single-sample variant calling","p":["When not using a population grouping file via --populations, variants will be called per-sample. Due to the nature of structural variant VCF files, there isn't an entirely fool-proof way"]},{"l":"Pooled-sample variant calling","p":["With the inclusion of a population grouping file via --populations, Harpy will merge the bam files of all samples within a population and call variants on these alignment pools. Preliminary work shows that this way identifies more variants and with fewer false"]},{"l":"Optional vcf file","p":["In order to get the best variant calling performance out of NAIBR, it requires phased bam files as input. Using --vcf is optional and not used by NAIBR directly. However, to use"]},{"l":"a phased input --vcf","p":["This file can be in vcf/vcf.gz/bcf format and most importantly it must be phased haplotypes. There are various ways to haplotype SNPs, but you can use to phase your SNPs into haplotypes using the haplotag barcode information. The resulting phased VCF file can then be used as input here."]},{"l":"NAIBR workflow"}]]